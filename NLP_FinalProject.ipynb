{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a36040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ravit\\anaconda3\\lib\\site-packages (4.48.1)\n",
      "Requirement already satisfied: torch in c:\\users\\ravit\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ravit\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ravit\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\ravit\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ravit\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch tqdm scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77a444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d34c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the external dataset for training\n",
    "def load_dream_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83d3a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data (provided dreams_data)\n",
    "test_dreams_data = {\n",
    "    \"dream\": [\n",
    "        \"Falling off a cliff\", \"Flying freely in the sky\", \"Teeth falling out one by one\",\n",
    "        \"Being naked in a crowded street\", \"Failing an important exam\", \"Being chased by a shadowy figure\",\n",
    "        \"Witnessing one's own death\", \"Running but moving in slow motion\", \"Meeting a deceased loved one\",\n",
    "        \"Drowning in deep water\", \"Driving a car but losing control\", \"Walking through a dark forest\",\n",
    "        \"Climbing a never-ending staircase\", \"Being trapped in a small room\", \"Losing one’s wallet or keys\",\n",
    "        \"Arguing with a stranger\", \"Being late for an important event\", \"Watching a plane crash\",\n",
    "        \"Standing in a burning building\", \"Falling into a deep abyss\", \"Seeing a baby crying\",\n",
    "        \"Being unable to speak\", \"Walking barefoot on sharp rocks\", \"Eating spoiled food\",\n",
    "        \"Discovering hidden treasure\", \"Losing hair suddenly\", \"Breaking a mirror\",\n",
    "        \"Finding oneself in a strange house\", \"Witnessing a friend get hurt\", \"Flying but struggling to stay in the air\",\n",
    "        \"Being in an endless maze\", \"Reuniting with a former lover\", \"Fighting with a family member\",\n",
    "        \"Losing eyesight or going blind\", \"Seeing a snake in a dream\", \"Falling from a high building\",\n",
    "        \"Receiving a gift from a stranger\", \"Being locked out of one’s home\", \"Discovering secret rooms in a familiar house\",\n",
    "        \"Crossing a turbulent river\", \"Seeing oneself in a mirror\", \"Losing a beloved pet\",\n",
    "        \"Sitting in an empty classroom\", \"Walking on thin ice\", \"Witnessing a sunrise\",\n",
    "        \"Fighting a wild animal\", \"Seeing a collapsing building\", \"Being unable to find one’s way home\",\n",
    "        \"Writing a letter that never gets sent\", \"Standing on a stage but forgetting lines\"],\n",
    "    \"interp\": [\n",
    "        \"Fear of losing control in life or anxieties about failure\",\n",
    "        \"A deep desire for liberation from constraints or limitations\",\n",
    "        \"Anxiety about physical appearance or communication breakdowns\",\n",
    "        \"Feeling exposed or vulnerable in social or professional settings\",\n",
    "        \"Fear of being judged or evaluated harshly by others\",\n",
    "        \"Avoidance of unresolved fears or challenges in waking life\",\n",
    "        \"Transition or significant changes happening in life\",\n",
    "        \"A feeling of helplessness or frustration in achieving goals\",\n",
    "        \"Processing grief or longing for past connections\",\n",
    "        \"Overwhelmed by emotions or unconscious conflicts surfacing\",\n",
    "        \"Concerns about control over life’s direction\",\n",
    "        \"Navigating through uncertainty or the unconscious mind\",\n",
    "        \"Struggling to achieve unattainable goals or self-improvement\",\n",
    "        \"Feeling confined or stuck in a situation\",\n",
    "        \"Anxiety about identity or security in waking life\",\n",
    "        \"Internal conflicts or repressed emotions surfacing\",\n",
    "        \"Pressure and fear of failing expectations\",\n",
    "        \"Fears of failure or witnessing a significant loss\",\n",
    "        \"Intense emotional stress or repressed anger\",\n",
    "        \"Existential fears or fear of losing stability\",\n",
    "        \"Anxiety about nurturing responsibilities or personal growth\",\n",
    "        \"Feeling silenced or unheard in waking life\",\n",
    "        \"Struggles or pain endured on the path to goals\",\n",
    "        \"Guilt or discomfort with choices made recently\",\n",
    "        \"A desire to uncover untapped potential or hidden talents\",\n",
    "        \"Concerns about aging or loss of vitality\",\n",
    "        \"Anxiety about self-image or fear of bad luck\",\n",
    "        \"Exploring unknown aspects of the self\",\n",
    "        \"Fear of losing someone or guilt over past actions\",\n",
    "        \"Ambivalence about freedom or personal achievements\",\n",
    "        \"Feeling lost or confused in waking life\",\n",
    "        \"Nostalgia or unresolved emotions from past relationships\",\n",
    "        \"Repressed anger or unresolved family tensions\",\n",
    "        \"Fear of ignorance or losing perspective\",\n",
    "        \"A symbol of transformation, fear, or temptation\",\n",
    "        \"Anxiety about failure or public humiliation\",\n",
    "        \"Expectation of unexpected opportunities or recognition\",\n",
    "        \"Feeling disconnected from personal identity or safety\",\n",
    "        \"Uncovering hidden aspects of the psyche\",\n",
    "        \"Overcoming emotional obstacles or major life changes\",\n",
    "        \"Reflecting on self-identity or inner conflicts\",\n",
    "        \"Processing grief or emotional dependence\",\n",
    "        \"Anxiety about learning or personal development\",\n",
    "        \"Fear of taking risks or instability in life\",\n",
    "        \"Hope and optimism for new beginnings\",\n",
    "        \"Struggles with primal instincts or internal aggression\",\n",
    "        \"Fear of losing stability or foundations in life\",\n",
    "        \"Longing for security or emotional grounding\",\n",
    "        \"Repressed communication or unspoken emotions\",\n",
    "        \"Anxiety about performance or public judgment\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729c843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset class for DREAM dataset\n",
    "class DreamDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=256):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for item in data:\n",
    "            # Extract dialogue, question, and answer\n",
    "            dialogue = ' '.join(item[0])  # Combine all dialogue turns\n",
    "            qa_data = item[1][0]  # Get the first (and only) QA pair\n",
    "            question = qa_data['question']\n",
    "            answer = qa_data['answer']\n",
    "            \n",
    "            # Format: \"Context: {dialogue} Question: {question} Answer: {answer}\"\n",
    "            text = f\"Context: {dialogue} Question: {question} Answer: {answer}\"\n",
    "            \n",
    "            encodings = tokenizer(text, truncation=True, max_length=max_length, \n",
    "                                padding='max_length', return_tensors='pt')\n",
    "            \n",
    "            self.input_ids.append(encodings['input_ids'].squeeze())\n",
    "            self.attn_masks.append(encodings['attention_mask'].squeeze())\n",
    "            self.labels.append(encodings['input_ids'].squeeze())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attn_masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f59a14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified generation function for dream interpretation\n",
    "def generate_interpretation(dream_text, model, tokenizer, device, max_length=150):\n",
    "    model.eval()\n",
    "    # Format dream as a dialogue-style context\n",
    "    context = f\"M: I had a dream last night. {dream_text}\\nW: Let me help you understand what that might mean.\"\n",
    "    prompt = f\"Context: {context} Question: What could this dream symbolize? Answer:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract only the answer part\n",
    "    interpretation = generated_text.split(\"Answer:\")[-1].strip()\n",
    "    return interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd76ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function remains the same\n",
    "def train(model, train_dataloader, val_dataloader, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=f'Epoch {epoch+1}'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            outputs = model(input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          labels=labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              labels=labels)\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "        print(f'Average validation loss: {avg_val_loss:.4f}\\n')\n",
    "        \n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac037b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model on test data\n",
    "def evaluate_test_set(model, tokenizer, device, test_data):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for dream, actual_interp in zip(test_data['dream'], test_data['interp']):\n",
    "        generated_interp = generate_interpretation(dream, model, tokenizer, device)\n",
    "        results.append({\n",
    "            'dream': dream,\n",
    "            'actual_interpretation': actual_interp,\n",
    "            'generated_interpretation': generated_interp\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9571a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ravit\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1:   8%|█████▋                                                                | 63/774 [10:23<2:01:24, 10.25s/it]"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load the training dataset\n",
    "    train_data = load_dream_dataset('dream_data.json')\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Split data into train and validation sets\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = DreamDataset(train_data, tokenizer)\n",
    "    val_dataset = DreamDataset(val_data, tokenizer)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    # Train the model\n",
    "    num_epochs = 5\n",
    "    train(model, train_dataloader, val_dataloader, optimizer, num_epochs, device)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save_pretrained('dream_interpreter_model')\n",
    "    tokenizer.save_pretrained('dream_interpreter_model')\n",
    "    \n",
    "    # Test the model on the provided dreams_data\n",
    "    print(\"\\nEvaluating model on test data:\")\n",
    "    test_results = evaluate_test_set(model, tokenizer, device, test_dreams_data)\n",
    "    \n",
    "    # Print test results\n",
    "    for result in test_results:\n",
    "        print(f\"\\nDream: {result['dream']}\")\n",
    "        print(f\"Actual Interpretation: {result['actual_interpretation']}\")\n",
    "        print(f\"Generated Interpretation: {result['generated_interpretation']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c86f920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
